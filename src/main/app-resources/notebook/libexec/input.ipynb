{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ewf-ext-03-02-01 - SeaEyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'ewf-ext-03-02-01 - SeaEyes'),\n",
    "                ('abstract', 'ewf-ext-03-02-01 - SeaEyes'),\n",
    "                ('id', 'ewf-ext-03-02-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shoreline Extension**\n",
    "\n",
    "Shoreline Extension: Extend the shoreline by this many pixels (Land-Sea-Mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shorelineExtension = dict([('id', 'shorelineExtension'),\n",
    "                           ('value', '10'),\n",
    "                           ('title', 'Shoreline Extension'),\n",
    "                           ('abstract', 'Shoreline Extension: Extend the shoreline by this many pixels (Land-Sea-Mask)'),\n",
    "                           ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability of false alarm**\n",
    "\n",
    "Probability of false alarm: The PFA value is computed by 10^(-x). where x is the given positive number (Adaptive Threshold Algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfa = dict([('id', 'pfa'),\n",
    "            ('value', '12.5'),\n",
    "            ('title', 'Probability of false alarm'),\n",
    "            ('abstract', 'Probability of false alarm: The PFA value is computed by 10^(-x). where x is the given positive number (Adaptive Threshold Algorithm)'),\n",
    "            ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimum Target Size (m)**\n",
    "\n",
    "Minimum Target Size (m): Target with dimension smaller than this threshold is eliminated (Object-Discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minTargetSizeInMeter = dict([('id', 'minTargetSizeInMeter'),\n",
    "                             ('value', '30.0'),\n",
    "                             ('title', 'Minimum Target Size'),\n",
    "                             ('abstract', 'Minimum Target Size (m): Target with dimension smaller than this threshold is eliminated (Object-Discrimination).'),\n",
    "                             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Target Size (m)**\n",
    "\n",
    "Maximum Target Size (m): Target with dimension larger than this threshold is eliminated (Object-Discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxTargetSizeInMeter = dict([('id', 'maxTargetSizeInMeter'),\n",
    "                             ('value', '600.0'),\n",
    "                             ('title', 'Maximum Target Size (m)'),\n",
    "                             ('abstract', 'Maximum Target Size (m): Target with dimension larger than this threshold is eliminated (Object-Discrimination).'),\n",
    "                             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AIS time interval**\n",
    "\n",
    "AIS time interval (min): Interval in minutes to search for AIS data around the S-1 acquisition time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aisTimeInterval = dict([('id', 'aisTimeInterval'),\n",
    "                        ('value', '10'),\n",
    "                        ('title', 'AIS time interval (min)'),\n",
    "                        ('abstract', 'AIS time interval: Interval in minutes to search for ais around the S-1 acquisition time.'),\n",
    "                        ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ['S1B_IW_GRDH_1SDV_20170703T194823_20170703T194848_006328_00B202_5554']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_references = ['https://catalog.terradue.com/sentinel1/search?uid=S1B_IW_GRDH_1SDV_20170703T194823_20170703T194848_006328_00B202_5554']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/workspace/data/S-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snappy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gdal\n",
    "\n",
    "import datetime\n",
    "\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()\n",
    "\n",
    "import shapely.wkt\n",
    "\n",
    "import lxml.etree as etree\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import psutil\n",
    "from snappy import jpy\n",
    "from snappy import ProductIO\n",
    "from snappy import GPF\n",
    "from snappy import HashMap\n",
    "\n",
    "import zipfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphProcessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root = etree.Element('graph')\n",
    "    \n",
    "        version = etree.SubElement(self.root, 'version')\n",
    "        version.text = '1.0'\n",
    "        self.pid = None\n",
    "        self.p = None\n",
    "   \n",
    "    def view_graph(self):\n",
    "        \n",
    "        print etree.tostring(self.root , pretty_print=True)\n",
    "        \n",
    "    def add_node(self, node_id, operator, parameters, source):\n",
    "    \n",
    "        xpath_expr = '/graph/node[@id=\"%s\"]' % node_id\n",
    "\n",
    "        if len(self.root.xpath(xpath_expr)) != 0:\n",
    "\n",
    "            node_elem = self.root.xpath(xpath_expr)[0]\n",
    "            operator_elem = self.root.xpath(xpath_expr + '/operator')[0]\n",
    "            sources_elem = self.root.xpath(xpath_expr + '/sources')[0]\n",
    "            parameters_elem = self.root.xpath(xpath_expr + '/parameters')\n",
    "\n",
    "            for key, value in parameters.iteritems():\n",
    "                p_elem = self.root.xpath(xpath_expr + '/parameters/%s' % key)[0]\n",
    "                p_elem.text = value\n",
    "        else:\n",
    "\n",
    "            node_elem = etree.SubElement(self.root, 'node')\n",
    "            operator_elem = etree.SubElement(node_elem, 'operator')\n",
    "            sources_elem = etree.SubElement(node_elem, 'sources')\n",
    "\n",
    "            if isinstance(source, list):\n",
    "\n",
    "                for index, s in enumerate(source):\n",
    "                    if index == 0:  \n",
    "                        source_product_elem = etree.SubElement(sources_elem, 'sourceProduct')\n",
    "\n",
    "                    else: \n",
    "                        source_product_elem = etree.SubElement(sources_elem, 'sourceProduct.%s' % str(index))\n",
    "\n",
    "                    source_product_elem.attrib['refid'] = s\n",
    "\n",
    "            elif source != '':\n",
    "                source_product_elem = etree.SubElement(sources_elem, 'sourceProduct')\n",
    "                source_product_elem.attrib['refid'] = source\n",
    "\n",
    "            parameters_elem = etree.SubElement(node_elem, 'parameters')\n",
    "            parameters_elem.attrib['class'] = 'com.bc.ceres.binding.dom.XppDomElement'\n",
    "\n",
    "            for key, value in parameters.iteritems():\n",
    "\n",
    "                parameter_elem = etree.SubElement(parameters_elem, key)\n",
    "                parameter_elem.text = value\n",
    "\n",
    "        node_elem.attrib['id'] = node_id\n",
    "\n",
    "        operator_elem.text = operator \n",
    "\n",
    "    def save_graph(self, filename):\n",
    "        \n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "            file.write(etree.tostring(self.root, pretty_print=True))\n",
    "     \n",
    "    def plot_graph(self):\n",
    "        \n",
    "        for node_id in self.root.xpath('/graph/node/@id'):\n",
    "            \n",
    "\n",
    "            xpath_expr = '/graph/node[@id=\"%s\"]' % node_id\n",
    "            \n",
    "            if len(self.root.xpath(xpath_expr + '/sources/sourceProduct')) != 0:\n",
    "                print(self.root.xpath(xpath_expr + '/sources/sourceProduct'))[0].attrib['refid']\n",
    "                print node_id\n",
    "            else:\n",
    "                print node_id\n",
    "        return True\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        fd, path = tempfile.mkstemp()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            self.save_graph(filename=path)\n",
    "            \n",
    "            options = ['/opt/snap6/bin/gpt',\n",
    "               '-x',\n",
    "               '-c',\n",
    "               '2048M',\n",
    "               path]\n",
    "            \n",
    "            #options = ['/workspace/temp/temp/snap6/snap6/bin/gpt',\n",
    "            #   '-x',\n",
    "            #   '-c',\n",
    "            #   '2048M',\n",
    "            #   path]\n",
    "\n",
    "            p = subprocess.Popen(options,\n",
    "                stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "            print p.pid\n",
    "            res, err = p.communicate()\n",
    "            print res, err\n",
    "            if p.returncode != 0:\n",
    "                raise Exception('An error occurred during the execution of gpt (see log)')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open('stdout.txt', 'wb') as file:\n",
    "                file.write(res)\n",
    "                file.close()\n",
    "            with open('stderr.txt', 'wb') as file:\n",
    "                file.write(err)\n",
    "                file.close()\n",
    "            \n",
    "            raise\n",
    "        finally:\n",
    "            os.remove(path)\n",
    "            \n",
    "            \n",
    "def get_operator_default_parameters(operator):\n",
    "    \n",
    "    op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "    op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "    #return op_params\n",
    "\n",
    "    parameters = dict()\n",
    "\n",
    "    for param in op_params:\n",
    "    \n",
    "        #print(param.getName(), param.getDefaultValue())\n",
    "    \n",
    "        parameters[param.getName()] = param.getDefaultValue()\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serial_processing(operators, **kwargs):\n",
    "   \n",
    "    options = dict()\n",
    "    \n",
    "    for operator in operators:\n",
    "            \n",
    "        print 'Getting default values for Operator {}'.format(operator)\n",
    "        parameters = get_operator_default_parameters(operator)\n",
    "        \n",
    "        options[operator] = parameters\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        \n",
    "        print 'Updating Operator {}'.format(key)\n",
    "        options[key.replace('_', '-')].update(value)\n",
    "    \n",
    "    mygraph = GraphProcessor()\n",
    "    \n",
    "    for index, operator in enumerate(operators):\n",
    "    \n",
    "        print 'Adding Operator {} to graph'.format(operator)\n",
    "        if index == 0:            \n",
    "            source_node_id = ''\n",
    "        \n",
    "        else:\n",
    "            source_node_id = operators[index - 1]\n",
    "        \n",
    "        mygraph.add_node(operator,\n",
    "                         operator, \n",
    "                         options[operator], source_node_id)\n",
    "    \n",
    "    mygraph.view_graph()\n",
    "    \n",
    "    mygraph.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_metadata (input_refs):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        \n",
    "        # since the search is by identifier \n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,\n",
    "                                       params =[],\n",
    "                                       output_fields='self,identifier,startdate,enclosure,title,startdate,enddate,track,swathIdentifier,wkt',\n",
    "                                       model='EOP')[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = pd.DataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    \n",
    "    return input_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_file(files, output_path):\n",
    "    with zipfile.ZipFile(output_path, 'w') as myzip:\n",
    "        for file in files:\n",
    "            myzip.write(file, arcname=os.path.basename(file))\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_folder = ''\n",
    "temp_folder = 'temp'\n",
    "shp_folder = 'shp'\n",
    "etc_path = \"/application/notebook/etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "        \n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)\n",
    "\n",
    "if not os.path.isdir(shp_folder):\n",
    "    os.mkdir(shp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get S-1 Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata = get_input_metadata(input_references)\n",
    "input_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operators definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = []\n",
    "for s1path in input_identifiers:\n",
    "    \n",
    "    read = dict()\n",
    "\n",
    "    s1meta = \"manifest.safe\"\n",
    "    \n",
    "    s1prd = \"%s/%s/%s.SAFE/%s\" % (data_path, s1path, s1path, s1meta)\n",
    "    \n",
    "    read['file'] =  s1prd\n",
    "    \n",
    "    reads.append(read)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landseamask = get_operator_default_parameters('Land-Sea-Mask')\n",
    "\n",
    "for p in landseamask:\n",
    "    if p == 'shorelineExtension':\n",
    "        landseamask[p] = shorelineExtension['value']\n",
    "\n",
    "landseamask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration = get_operator_default_parameters('Calibration')\n",
    "\n",
    "calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivethresholding = get_operator_default_parameters('AdaptiveThresholding')\n",
    "\n",
    "for p in adaptivethresholding:\n",
    "    if p == 'pfa':\n",
    "        adaptivethresholding[p] = pfa['value']\n",
    "\n",
    "adaptivethresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdiscrimination = get_operator_default_parameters('Object-Discrimination')\n",
    "\n",
    "for p in objectdiscrimination:\n",
    "    if p == 'minTargetSizeInMeter':\n",
    "        objectdiscrimination[p] = minTargetSizeInMeter['value']\n",
    "    elif p == 'maxTargetSizeInMeter':\n",
    "        objectdiscrimination[p] = maxTargetSizeInMeter['value']\n",
    "\n",
    "objectdiscrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writes = []\n",
    "for s1path in input_identifiers:\n",
    "    \n",
    "    write = dict()\n",
    "    \n",
    "    output_path = os.path.join(temp_folder, s1path + '_temp')\n",
    "    \n",
    "    write['file'] = output_path\n",
    "\n",
    "    writes.append(write)\n",
    "writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vessel detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "operators = ['Read',\n",
    "             'Land-Sea-Mask',\n",
    "             'Calibration',\n",
    "             'AdaptiveThresholding',\n",
    "             'Object-Discrimination',\n",
    "             'Write']\n",
    "\n",
    "for r,w in zip(reads,writes):\n",
    "    \n",
    "    serial_processing(operators,\n",
    "                      Read=r,\n",
    "                      Land_Sea_Mask=landseamask,\n",
    "                      Calibration=calibration,\n",
    "                      AdaptiveThresholding=adaptivethresholding,\n",
    "                      Object_Discrimination=objectdiscrimination,\n",
    "                      Write=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detected vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipdetections_path = os.path.join(writes[0]['file'] + '.data', 'vector_data', 'ShipDetections.csv')\n",
    "shipdetections_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pd dataframe\n",
    "shipdetections_raw = pd.read_csv(shipdetections_path,sep='\\t', skiprows=[0])\n",
    "shipdetections_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and add geometry\n",
    "shipdetections_gdf = gpd.GeoDataFrame(shipdetections_raw[['ShipDetections', 'Detected_width:Double', 'Detected_length:Double']], geometry=gpd.points_from_xy(shipdetections_raw['Detected_lon:Double'], shipdetections_raw['Detected_lat:Double']), crs=\"EPSG:4326\")\n",
    "\n",
    "shipdetections_gdf['TIMESTAMP(UTC)'] = input_metadata.iloc[0]['startdate'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "shipdetections_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipdetections_gdf = shipdetections_gdf.rename(columns={\"ShipDetections\": \"detection\",\n",
    "                                                        \"Detected_width:Double\": \"width\",\n",
    "                                                        \"Detected_length:Double\": \"length\",\n",
    "                                                        \"TIMESTAMP(UTC)\":\"timestamp\"})\n",
    "shipdetections_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipdetections_gdf.to_file(os.path.join(shp_folder, 'shipdetections.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_rel_path = 'ais_2017_07.csv'\n",
    "ais_data = os.path.join(etc_path, ais_rel_path)\n",
    "    \n",
    "if not(os.path.isfile(ais_data)): # when running locally\n",
    "    ais_data = os.path.join(os.path.dirname(os.getcwd()), 'etc', ais_rel_path)\n",
    "        \n",
    "print(os.path.isfile(ais_data), ais_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AIS data (first to DataFrame, then convert to GeoDataFrame).\n",
    "ais_df = pd.read_csv(ais_data, decimal=\",\", usecols=['MMSI', 'IMO', 'STATUS', 'SPEED(KNOTS x10)', 'LON', 'LAT',\n",
    "                                                     'COURSE', 'HEADING', 'TIMESTAMP(UTC)'],\n",
    "                     dtype={'LON': float, 'LAT': float})\n",
    "\n",
    "ais_df['TIMESTAMP(UTC)'] = pd.to_datetime(ais_df['TIMESTAMP(UTC)'])\n",
    "\n",
    "print('Number of rows in %s: %d' % (ais_data.split('\\\\')[-1], len(ais_df.index)))\n",
    "\n",
    "# add geometry\n",
    "ais_gdf = gpd.GeoDataFrame(ais_df, geometry=gpd.points_from_xy(ais_df.LON, ais_df.LAT), crs=\"EPSG:4326\")\n",
    "ais_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by AOI\n",
    "AOI = shapely.wkt.loads(input_metadata.iloc[0]['wkt'])\n",
    "ais_gdf_aoi = ais_gdf[ais_gdf['geometry'].within(AOI)]\n",
    "ais_gdf_aoi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter in time\n",
    "aisti = int(aisTimeInterval['value'])\n",
    "min_date = input_metadata.iloc[0]['startdate'] - pd.Timedelta(minutes=aisti)\n",
    "max_date = input_metadata.iloc[0]['enddate'] + pd.Timedelta(minutes=aisti)\n",
    "print(min_date, max_date)\n",
    "ais_gdf_aoi = ais_gdf_aoi[(ais_gdf_aoi['TIMESTAMP(UTC)'] > min_date) & (ais_gdf_aoi['TIMESTAMP(UTC)'] < max_date)]\n",
    "\n",
    "ais_gdf_aoi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ais_gdf_aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way\n",
    "sorted = ais_gdf_aoi.sort_values(['MMSI', 'TIMESTAMP(UTC)'], ascending = [True, False])\n",
    "\n",
    "first = sorted.groupby('MMSI').first().reset_index()\n",
    "\n",
    "first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first['TIMESTAMP(UTC)'] = first['TIMESTAMP(UTC)'].dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = first.rename(columns={\"TIMESTAMP(UTC)\": \"TIMESTAMP\",\n",
    "                    \"SPEED(KNOTS x10)\": \"SPEED\"})\n",
    "\n",
    "first = gpd.GeoDataFrame(first, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to shp\n",
    "first.to_file(os.path.join(shp_folder, 'ais.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output shp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_wkt = input_metadata.iloc[0]['wkt']\n",
    "track = input_metadata.iloc[0]['track']\n",
    "\n",
    "startdate_iso = pd.to_datetime(input_metadata.iloc[0]['startdate']).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "enddate_iso = pd.to_datetime(input_metadata.iloc[0]['enddate']).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "startdatetime = input_metadata.iloc[0]['startdate'].strftime('%Y%m%dT%H%M')\n",
    "\n",
    "s1track = 'S1{0:03}'.format(int(track))\n",
    "\n",
    "output_name = '_'.join([s1track,'AIS', 'vesseldetection', startdatetime])\n",
    "                        \n",
    "output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_list = glob.glob(shp_folder + '/*')\n",
    "\n",
    "create_zip_file(zip_list, output_name + '.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_properties_file(output_name + '.zip', startdate_iso, enddate_iso, area_wkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terrain-correction and geotiff output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = []\n",
    "for e in writes:\n",
    "    \n",
    "    read = dict()\n",
    " \n",
    "    read['file'] =  e['file'] + '.dim'\n",
    "    \n",
    "    reads.append(read)\n",
    "    \n",
    "reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terraincorrection = get_operator_default_parameters('Terrain-Correction')\n",
    "\n",
    "for p in terraincorrection:\n",
    "    if p == 'nodataValueAtSea':\n",
    "        terraincorrection[p] = 'false'\n",
    "    elif p == 'sourceBandNames':\n",
    "        terraincorrection[p] = 'Sigma0_VV'\n",
    "\n",
    "terraincorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writes_tc = []\n",
    "for index, row in input_metadata.iterrows():\n",
    "    \n",
    "    area_wkt = row['wkt']\n",
    "    track = row['track']\n",
    "\n",
    "    startdate_iso = pd.to_datetime(row['startdate']).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    enddate_iso = pd.to_datetime(row['enddate']).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    startdatetime = row['startdate'].strftime('%Y%m%dT%H%M')\n",
    "\n",
    "    s1track = 'S1{0:03}'.format(int(track))\n",
    "\n",
    "    output_name = '_'.join([s1track,'Sigma0VV', startdatetime])\n",
    "                        \n",
    "    #print(output_name)\n",
    "    \n",
    "    \n",
    "    write = dict()\n",
    "    \n",
    "    output_path = output_name + '.tif'\n",
    "    \n",
    "    write['file'] = output_path\n",
    "    write['formatName'] = 'GeoTIFF-BigTIFF'\n",
    "\n",
    "    writes_tc.append(write)\n",
    "    \n",
    "    # properties file\n",
    "    write_properties_file(output_name + '.tif', startdate_iso, enddate_iso, area_wkt)\n",
    "    \n",
    "writes_tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = ['Read',\n",
    "             'Terrain-Correction',\n",
    "             'Write']\n",
    "\n",
    "for r,w in zip(reads,writes_tc):\n",
    "    \n",
    "    serial_processing(operators,\n",
    "                      Read=r,\n",
    "                      Terrain_Correction=terraincorrection,\n",
    "                      Write=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_folder)\n",
    "shutil.rmtree(shp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"license\">License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is licenced under a [Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "YOU ARE FREE TO:\n",
    "\n",
    "* Share - copy and redistribute the material in any medium or format.\n",
    "* Adapt - remix, transform, and built upon the material for any purpose, even commercially.\n",
    "\n",
    "UNDER THE FOLLOWING TERMS:\n",
    "\n",
    "* Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n",
    "* ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
