{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ewf-ext-03-02-01 - SeaEyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'ewf-ext-03-02-01 - SeaEyes'),\n",
    "                ('abstract', 'ewf-ext-03-02-01 - SeaEyes'),\n",
    "                ('id', 'ewf-ext-03-02-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive Thresholding**\n",
    "\n",
    "Adaptive Thresholding: Extend the shoreline by this many pixels (Land-Sea-Mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shorelineExtension = dict([('id', 'shorelineExtension'),\n",
    "                           ('value', '10'),\n",
    "                           ('title', 'Shoreline Extension'),\n",
    "                           ('abstract', 'Shoreline Extension: Extend the shoreline by this many pixels (Land-Sea-Mask)'),\n",
    "                           ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability of false alarm**\n",
    "\n",
    "Probability of false alarm: The PFA value is computed by 10^(-x). where x is the given positive number (Adaptive Threshold Algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfa = dict([('id', 'pfa'),\n",
    "            ('value', '12.5'),\n",
    "            ('title', 'Probability of false alarm'),\n",
    "            ('abstract', 'Probability of false alarm: The PFA value is computed by 10^(-x). where x is the given positive number (Adaptive Threshold Algorithm)'),\n",
    "            ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimum Target Size (m)**\n",
    "\n",
    "Minimum Target Size (m): Target with dimension smaller than this threshold is eliminated (Object-Discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minTargetSizeInMeter = dict([('id', 'minTargetSizeInMeter'),\n",
    "                             ('value', '30.0'),\n",
    "                             ('title', 'Minimum Target Size'),\n",
    "                             ('abstract', 'Minimum Target Size (m): Target with dimension smaller than this threshold is eliminated (Object-Discrimination).'),\n",
    "                             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Target Size (m)**\n",
    "\n",
    "Maximum Target Size (m): Target with dimension larger than this threshold is eliminated (Object-Discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxTargetSizeInMeter = dict([('id', 'maxTargetSizeInMeter'),\n",
    "                             ('value', '600.0'),\n",
    "                             ('title', 'Maximum Target Size (m)'),\n",
    "                             ('abstract', 'Maximum Target Size (m): Target with dimension larger than this threshold is eliminated (Object-Discrimination).'),\n",
    "                             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ['S1B_IW_GRDH_1SDV_20170703T194823_20170703T194848_006328_00B202_5554']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_references = ['https://catalog.terradue.com/sentinel1/search?uid=S1B_IW_GRDH_1SDV_20170703T194823_20170703T194848_006328_00B202_5554']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/workspace/data/S-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snappy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import cioppy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gdal\n",
    "\n",
    "import datetime\n",
    "\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import shapely.wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as etree\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import psutil\n",
    "from snappy import jpy\n",
    "from snappy import ProductIO\n",
    "from snappy import GPF\n",
    "from snappy import HashMap\n",
    "\n",
    "\n",
    "class GraphProcessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root = etree.Element('graph')\n",
    "    \n",
    "        version = etree.SubElement(self.root, 'version')\n",
    "        version.text = '1.0'\n",
    "        self.pid = None\n",
    "        self.p = None\n",
    "   \n",
    "    def view_graph(self):\n",
    "        \n",
    "        print etree.tostring(self.root , pretty_print=True)\n",
    "        \n",
    "    def add_node(self, node_id, operator, parameters, source):\n",
    "    \n",
    "        xpath_expr = '/graph/node[@id=\"%s\"]' % node_id\n",
    "\n",
    "        if len(self.root.xpath(xpath_expr)) != 0:\n",
    "\n",
    "            node_elem = self.root.xpath(xpath_expr)[0]\n",
    "            operator_elem = self.root.xpath(xpath_expr + '/operator')[0]\n",
    "            sources_elem = self.root.xpath(xpath_expr + '/sources')[0]\n",
    "            parameters_elem = self.root.xpath(xpath_expr + '/parameters')\n",
    "\n",
    "            for key, value in parameters.iteritems():\n",
    "                p_elem = self.root.xpath(xpath_expr + '/parameters/%s' % key)[0]\n",
    "                p_elem.text = value\n",
    "        else:\n",
    "\n",
    "            node_elem = etree.SubElement(self.root, 'node')\n",
    "            operator_elem = etree.SubElement(node_elem, 'operator')\n",
    "            sources_elem = etree.SubElement(node_elem, 'sources')\n",
    "\n",
    "            if isinstance(source, list):\n",
    "\n",
    "                for index, s in enumerate(source):\n",
    "                    if index == 0:  \n",
    "                        source_product_elem = etree.SubElement(sources_elem, 'sourceProduct')\n",
    "\n",
    "                    else: \n",
    "                        source_product_elem = etree.SubElement(sources_elem, 'sourceProduct.%s' % str(index))\n",
    "\n",
    "                    source_product_elem.attrib['refid'] = s\n",
    "\n",
    "            elif source != '':\n",
    "                source_product_elem = etree.SubElement(sources_elem, 'sourceProduct')\n",
    "                source_product_elem.attrib['refid'] = source\n",
    "\n",
    "            parameters_elem = etree.SubElement(node_elem, 'parameters')\n",
    "            parameters_elem.attrib['class'] = 'com.bc.ceres.binding.dom.XppDomElement'\n",
    "\n",
    "            for key, value in parameters.iteritems():\n",
    "\n",
    "                parameter_elem = etree.SubElement(parameters_elem, key)\n",
    "                parameter_elem.text = value\n",
    "\n",
    "        node_elem.attrib['id'] = node_id\n",
    "\n",
    "        operator_elem.text = operator \n",
    "\n",
    "    def save_graph(self, filename):\n",
    "        \n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "            file.write(etree.tostring(self.root, pretty_print=True))\n",
    "     \n",
    "    def plot_graph(self):\n",
    "        \n",
    "        for node_id in self.root.xpath('/graph/node/@id'):\n",
    "            \n",
    "\n",
    "            xpath_expr = '/graph/node[@id=\"%s\"]' % node_id\n",
    "            \n",
    "            if len(self.root.xpath(xpath_expr + '/sources/sourceProduct')) != 0:\n",
    "                print(self.root.xpath(xpath_expr + '/sources/sourceProduct'))[0].attrib['refid']\n",
    "                print node_id\n",
    "            else:\n",
    "                print node_id\n",
    "        return True\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        fd, path = tempfile.mkstemp()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            self.save_graph(filename=path)\n",
    "            \n",
    "            options = ['/opt/snap6/bin/gpt',\n",
    "               '-x',\n",
    "               '-c',\n",
    "               '2048M',\n",
    "               path]\n",
    "            \n",
    "            options = ['/workspace/temp/temp/snap6/snap6/bin/gpt',\n",
    "               '-x',\n",
    "               '-c',\n",
    "               '2048M',\n",
    "               path]\n",
    "\n",
    "            p = subprocess.Popen(options,\n",
    "                stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "            print p.pid\n",
    "            res, err = p.communicate()\n",
    "            print res, err\n",
    "            if p.returncode != 0:\n",
    "                raise Exception('An error occurred during the execution of gpt (see log)')\n",
    "            \n",
    "        except Exception as e:\n",
    "            with open('stdout.txt', 'wb') as file:\n",
    "                file.write(res)\n",
    "                file.close()\n",
    "            with open('stderr.txt', 'wb') as file:\n",
    "                file.write(err)\n",
    "                file.close()\n",
    "            \n",
    "            raise\n",
    "        finally:\n",
    "            os.remove(path)\n",
    "            \n",
    "            \n",
    "def get_operator_default_parameters(operator):\n",
    "    \n",
    "    op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "    op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "    #return op_params\n",
    "\n",
    "    parameters = dict()\n",
    "\n",
    "    for param in op_params:\n",
    "    \n",
    "        #print(param.getName(), param.getDefaultValue())\n",
    "    \n",
    "        parameters[param.getName()] = param.getDefaultValue()\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vessel_detection_processing(**kwargs):\n",
    "   \n",
    "    options = dict()\n",
    "    \n",
    "    operators = ['Read',\n",
    "                 'Land-Sea-Mask',\n",
    "                 'Calibration',\n",
    "                 'AdaptiveThresholding',\n",
    "                 'Object-Discrimination',\n",
    "                 'Write']\n",
    "    \n",
    "    for operator in operators:\n",
    "            \n",
    "        print 'Getting default values for Operator {}'.format(operator)\n",
    "        parameters = get_operator_default_parameters(operator)\n",
    "        \n",
    "        options[operator] = parameters\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        \n",
    "        print 'Updating Operator {}'.format(key)\n",
    "        options[key.replace('_', '-')].update(value)\n",
    "    \n",
    "    mygraph = GraphProcessor()\n",
    "    \n",
    "    for index, operator in enumerate(operators):\n",
    "    \n",
    "        print 'Adding Operator {} to graph'.format(operator)\n",
    "        if index == 0:            \n",
    "            source_node_id = ''\n",
    "        \n",
    "        else:\n",
    "            source_node_id = operators[index - 1]\n",
    "        \n",
    "        mygraph.add_node(operator,\n",
    "                         operator, \n",
    "                         options[operator], source_node_id)\n",
    "    \n",
    "    mygraph.view_graph()\n",
    "    \n",
    "    mygraph.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_folder = ''\n",
    "temp_folder = 'temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "        \n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operators definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = []\n",
    "for s1path in input_identifiers:\n",
    "    \n",
    "    read = dict()\n",
    "\n",
    "    s1meta = \"manifest.safe\"\n",
    "    \n",
    "    s1prd = \"%s/%s/%s.SAFE/%s\" % (data_path, s1path, s1path, s1meta)\n",
    "    \n",
    "    read['file'] =  s1prd\n",
    "    #read['formatName'] = 'Sen3_SLSTRL1B_500m'\n",
    "    \n",
    "    reads.append(read)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landseamask = get_operator_default_parameters('Land-Sea-Mask')\n",
    "\n",
    "for p in landseamask:\n",
    "    if p == 'shorelineExtension':\n",
    "        landseamask[p] = shorelineExtension['value']\n",
    "\n",
    "landseamask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration = get_operator_default_parameters('Calibration')\n",
    "\n",
    "#for p in LandSeaMask:\n",
    "#    if p == 'shorelineExtension':\n",
    "#        LandSeaMask[p] = '10'\n",
    "\n",
    "calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivethresholding = get_operator_default_parameters('AdaptiveThresholding')\n",
    "\n",
    "for p in adaptivethresholding:\n",
    "    if p == 'pfa':\n",
    "        adaptivethresholding[p] = pfa['value']\n",
    "\n",
    "adaptivethresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdiscrimination = get_operator_default_parameters('Object-Discrimination')\n",
    "\n",
    "for p in objectdiscrimination:\n",
    "    if p == 'minTargetSizeInMeter':\n",
    "        objectdiscrimination[p] = minTargetSizeInMeter['value']\n",
    "    elif p == 'maxTargetSizeInMeter':\n",
    "        objectdiscrimination[p] = maxTargetSizeInMeter['value']\n",
    "\n",
    "objectdiscrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writes = []\n",
    "for s1path in input_identifiers:\n",
    "    \n",
    "    write = dict()\n",
    "    \n",
    "    output_path = os.path.join(temp_folder, s1path + '_temp')\n",
    "    \n",
    "    write['file'] = output_path\n",
    "\n",
    "    writes.append(write)\n",
    "writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r,w in zip(reads,writes):\n",
    "    \n",
    "    vessel_detection_processing(Read=r,\n",
    "                                Land_Sea_Mask=landseamask,\n",
    "                                Calibration=calibration,\n",
    "                                AdaptiveThresholding=adaptivethresholding,\n",
    "                                Object_Discrimination=objectdiscrimination,\n",
    "                                Write=w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detected vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipdetections_path = os.path.join(writes[0]['file'] + '.data', 'vector_data', 'ShipDetections.csv')\n",
    "shipdetections_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pd dataframe\n",
    "shipdetections_raw = pd.read_csv(shipdetections_path,sep='\\t', skiprows=[0])\n",
    "shipdetections_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and add geometry\n",
    "shipdetections_gdf = gpd.GeoDataFrame(shipdetections_raw[['ShipDetections', 'Detected_width:Double', 'Detected_length:Double']], geometry=gpd.points_from_xy(shipdetections_raw['Detected_lon:Double'], shipdetections_raw['Detected_lat:Double']), crs=\"EPSG:4326\")\n",
    "\n",
    "shipdetections_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipdetections_gdf.to_file(\"result2.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data = '/workspace/Better_3rd_phase/Applications/EXT-03-02-01/ewf-ext-03-02-01/src/main/app-resources/notebook/etc/ais_2017_07.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AIS data (first to DataFrame, then convert to GeoDataFrame).\n",
    "ais_df = pd.read_csv(ais_data, decimal=\",\", usecols=['MMSI', 'IMO', 'STATUS', 'SPEED(KNOTS x10)', 'LON', 'LAT',\n",
    "                                                     'COURSE', 'HEADING', 'TIMESTAMP(UTC)'],\n",
    "                     dtype={'LON': float, 'LAT': float})\n",
    "\n",
    "ais_df['TIMESTAMP(UTC)'] = pd.to_datetime(ais_df['TIMESTAMP(UTC)'])\n",
    "\n",
    "print('Number of rows in %s: %d' % (ais_data.split('\\\\')[-1], len(ais_df.index)))\n",
    "\n",
    "\n",
    "\n",
    "# add geometry\n",
    "ais_gdf = gpd.GeoDataFrame(ais_df, geometry=gpd.points_from_xy(ais_df.LON, ais_df.LAT), crs=\"EPSG:4326\")\n",
    "ais_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by AOI\n",
    "AOI = shapely.wkt.loads('POLYGON ((-29.97968971227660973 39.49718800212976078, -26.71593511606884519 39.49718800212976078, -26.75288328130893234 37.42809074868483776, -29.99200576735664114 37.46503891392492847, -29.97968971227660973 39.49718800212976078))')\n",
    "ais_gdf_aoi = ais_gdf[ais_gdf['geometry'].within(AOI)]\n",
    "ais_gdf_aoi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter in time\n",
    "ais_gdf_aoi = ais_gdf_aoi[(ais_gdf_aoi['TIMESTAMP(UTC)'] > '2017-07-3 19:40:00') & (ais_gdf_aoi['TIMESTAMP(UTC)'] < '2017-07-3 19:50:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ais_gdf_aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way\n",
    "sorted = ais_gdf_aoi.sort_values(['MMSI', 'TIMESTAMP(UTC)'], ascending = [True, False])\n",
    "\n",
    "first = sorted.groupby('MMSI').first().reset_index()\n",
    "\n",
    "res = gpd.GeoDataFrame(first)\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_file(\"ais.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"license\">License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is licenced under a [Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "YOU ARE FREE TO:\n",
    "\n",
    "* Share - copy and redistribute the material in any medium or format.\n",
    "* Adapt - remix, transform, and built upon the material for any purpose, even commercially.\n",
    "\n",
    "UNDER THE FOLLOWING TERMS:\n",
    "\n",
    "* Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n",
    "* ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
